{"version":3,"sources":["webpack:///webpack/bootstrap 8d5dc9a55c94b1145e88","webpack:///./src/linear_regression/bgdVsgd/Regressor.es6","webpack:///./src/linear_regression/bgdVsgd/Worker.es6","webpack:///./src/linear_regression/bgdVsgd/Batch.es6","webpack:///./src/linear_regression/bgdVsgd/Stochastic.es6"],"names":["Regressor","rn","m","batchNum","objectName","real_numbers","measurements","object_name","costHistory","epocPost","setup","y","b","epoch","training_examples","x","points","error","i","length","Math","pow","line","map","n","postMessage","action","epocNum","ni","lineData","getLineData","c","a","LEARNING_RATE","bestB","bestM","bestC","Infinity","timer","hc","pc","NUM_POINTS","TIME_INTERVAL","RANDOM_ALPHA","MAX_ITERATION","PRECISION","Worker","e","console","log","data","workerType","worker","realNumbers","start","startY","w","onmessage","handleMessage","Batch","N","bGrad","mGrad","diff","cost","push","ccost","abs","drawLine","clearTimeout","drawCost","setTimeout","Stochastic","ret","tmpData","slice","index","round","random","item","splice","d","shuffle"],"mappings":";AAAA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;AAEA;AACA;AACA;AACA;AACA;AACA;;AAEA;AACA;;AAEA;AACA;;AAEA;AACA;AACA;;;AAGA;AACA;;AAEA;AACA;;AAEA;AACA,mDAA2C,cAAc;;AAEzD;AACA;AACA;AACA;AACA;AACA;AACA;AACA,aAAK;AACL;AACA;;AAEA;AACA;AACA;AACA,mCAA2B,0BAA0B,EAAE;AACvD,yCAAiC,eAAe;AAChD;AACA;AACA;;AAEA;AACA,8DAAsD,+DAA+D;;AAErH;AACA;;AAEA;AACA;;;;;;;;;;;;;;;;;;IChEMA,S;AAUF,uBAAYC,EAAZ,EAAgBC,CAAhB,EAAmBC,QAAnB,EAA6BC,UAA7B,EAAwC;AAAA;;AACpC,aAAKC,YAAL,GAAoBJ,EAApB;AACA,aAAKK,YAAL,GAAoBJ,CAApB;AACA,aAAKC,QAAL,GAAgBA,QAAhB;AACA,aAAKI,WAAL,GAAmBH,UAAnB;AACA,aAAKI,WAAL,GAAmB,EAAnB;AACA,aAAKC,QAAL,GAAgB,CAAhB;AACA,aAAKC,KAAL;AACH;;;;8BAEKC,C,EAAE;AACJ,iBAAKC,CAAL,GAASD,CAAT;AACA,iBAAKE,KAAL,CAAW,KAAKC,iBAAhB;AACH;;;6BAEIC,C,EAAE;AACH,mBAAO,KAAKH,CAAL,GAAS,KAAKV,CAAL,GAASa,CAAzB;AACH;;;6BAEIC,M,EAAQ;AACT,gBAAIC,QAAQ,CAAZ;AAAA,gBAAeC,UAAf;AAAA,gBAAkBH,UAAlB;AAAA,gBAAqBJ,UAArB;AACA,iBAAKO,IAAI,CAAT,EAAYA,IAAIF,OAAOG,MAAvB,EAA+BD,GAA/B,EAAoC;AAChCH,oBAAIC,OAAOE,CAAP,EAAU,CAAV,CAAJ;AACAP,oBAAIK,OAAOE,CAAP,EAAU,CAAV,CAAJ;AACAD,yBAASG,KAAKC,GAAL,CAAUV,IAAI,KAAKW,IAAL,CAAUP,CAAV,CAAd,EAA6B,CAA7B,CAAT;AACH;AACD,mBAAOE,QAAQD,OAAOG,MAAtB;AACH;;;sCAEY;AAAA;;AACT,mBAAO,KAAKd,YAAL,CAAkBkB,GAAlB,CAAsB,UAACC,CAAD,EAAIN,CAAJ,EAAU;AACnC,uBAAO,MAAKI,IAAL,CAAUJ,CAAV,CAAP;AACH,aAFM,CAAP;AAGH;;;mCAES;AACNO,wBAAY;AACRC,wBAAQ,WADA;AAERtB,4BAAY,KAAKG,WAFT;AAGRJ,0BAAU,KAAKA,QAHP;AAIRK,6BAAa,KAAKA,WAJV;AAKRmB,yBAAS,KAAKC;AALN,aAAZ;AAOH;;;mCAES;AACN,gBAAG,KAAKnB,QAAL,GAAgB,EAAhB,IAAsB,CAAzB,EAA4B;AACxB,oBAAMoB,WAAW,KAAKC,WAAL,EAAjB;;AAEAL,4BAAY;AACRC,4BAAQ,WADA;AAERG,8BAAUA,QAFF;AAGR1B,8BAAU,KAAKA,QAHP;AAIRwB,6BAAS,KAAKC;AAJN,iBAAZ;AAMH;AACD,iBAAKnB,QAAL;AACH;;;gCAEM;AACH;;;AAGA,iBAAKK,iBAAL,GAAyB,KAAKR,YAAL,CACpBiB,GADoB,CAChB,UAACC,CAAD,EAAIN,CAAJ,EAAU;AACX,uBAAO,CAACA,CAAD,EAAIM,CAAJ,CAAP;AACH,aAHoB,CAAzB;AAIA;;;;AAIA,iBAAKI,EAAL,GAAU,CAAV;AACA;;;;AAIA,iBAAKG,CAAL,GAAS,CAAT;AACA;;;;AAIA,iBAAKC,CAAL,GAAShC,UAAUiC,aAAnB;AACA;;;;AAIA,iBAAKC,KAAL,GAAa,CAAb;AACA;;;;AAIA,iBAAKC,KAAL,GAAa,CAAb;AACA;;;;AAIA,iBAAKC,KAAL,GAAaC,QAAb;AACA;;;;AAIA,iBAAKnC,CAAL,GAAS,CAAT;AACA;;;;AAIA,iBAAKoC,KAAL,GAAa,IAAb;AACA;;;;AAIA,iBAAKC,EAAL,GAAU,IAAV;AACA;;;;AAIA,iBAAKC,EAAL,GAAUH,QAAV;AACH;;;;;;AA/HCrC,S,CAGKyC,U,GAAa,E;AAHlBzC,S,CAIK0C,a,GAAgB,E;AAJrB1C,S,CAKK2C,Y,GAAe,E;AALpB3C,S,CAMKiC,a,GAAgB,K;AANrBjC,S,CAOK4C,a,GAAgB,I;AAPrB5C,S,CAQK6C,S,GAAY,K;kBA0HR7C,S;;;;;;;;;;;;AClIf;;;;AACA;;;;;;;;IAGM8C,M;;;;;;;sCAEYC,C,EAAE;AACZC,oBAAQC,GAAR,CAAY,mCAAZ;AACAD,oBAAQC,GAAR,CAAYF,EAAEG,IAAd;AACA,oBAAOH,EAAEG,IAAF,CAAOxB,MAAd;AACI,qBAAK,iBAAL;AACI,4BAAOqB,EAAEG,IAAF,CAAOC,UAAd;AACI,6BAAK,OAAL;AACI,iCAAKC,MAAL,GAAc,oBAAUL,EAAEG,IAAF,CAAO5C,YAAjB,EAA+ByC,EAAEG,IAAF,CAAOG,WAAtC,EAAmDN,EAAEG,IAAF,CAAO/C,QAA1D,CAAd;AACA;;AAEJ,6BAAK,YAAL;AACI,iCAAKiD,MAAL,GAAc,yBAAeL,EAAEG,IAAF,CAAO5C,YAAtB,EAAoCyC,EAAEG,IAAF,CAAOG,WAA3C,EAAwDN,EAAEG,IAAF,CAAO/C,QAA/D,CAAd;AACA;AAPR;AASA,yBAAKiD,MAAL,CAAYE,KAAZ,CAAkBP,EAAEG,IAAF,CAAOK,MAAzB;AACA;AAZR;AAcH;;;;;;AAGL,IAAMC,IAAI,IAAIV,MAAJ,EAAV;;AAEAW,YAAY,mBAACV,CAAD,EAAO;AACfS,MAAEE,aAAF,CAAgBX,CAAhB;AACH,CAFD;;;;;;;;;;;;;;;;AC5BA;;;;;;;;;;;;IAEMY,K;;;AAEF,mBAAY1D,EAAZ,EAAgBC,CAAhB,EAAmBC,QAAnB,EAA4B;AAAA;;AAAA,6GAClBF,EADkB,EACdC,CADc,EACXC,QADW,EACD,OADC;AAE3B;;;;8BAEK+C,I,EAAK;AAAA;;AACP;AACA,iBAAKtB,EAAL;AACA,gBAAMgC,IAAIV,KAAK/B,MAAf;AACA,gBAAI0C,QAAQ,CAAZ;AACA,gBAAIC,QAAQ,CAAZ;AACA,iBAAI,IAAI5C,IAAI,CAAZ,EAAeA,IAAI0C,CAAnB,EAAsB1C,GAAtB,EAA2B;AACvB,oBAAMH,IAAImC,KAAKhC,CAAL,EAAQ,CAAR,CAAV;AACA,oBAAMP,IAAIuC,KAAKhC,CAAL,EAAQ,CAAR,CAAV;AACA,oBAAM6C,OAAQpD,IAAI,KAAKW,IAAL,CAAUP,CAAV,CAAlB;;AAEA;AACA8C,yBAASE,IAAT;AACAD,yBAASC,OAAOhD,CAAhB;AACH;AACD,iBAAKH,CAAL,GAAS,KAAKA,CAAL,GAAU,KAAKoB,CAAL,GAAS6B,KAA5B;AACA,iBAAK3D,CAAL,GAAS,KAAKA,CAAL,GAAU,EAAE,IAAE0D,CAAJ,KAAU,KAAK5B,CAAL,GAAS8B,KAAnB,CAAnB;AACA,iBAAKtB,EAAL,GAAU,KAAKT,CAAf;AACA;AACA,iBAAKA,CAAL,GAAS,KAAKiC,IAAL,CAAU,KAAKlD,iBAAf,CAAT;AACA,iBAAKN,WAAL,CAAiByD,IAAjB,CAAsB,KAAKlC,CAA3B;AACA;AACA,gBAAG,KAAKA,CAAL,GAAS,KAAKK,KAAjB,EAAuB;AACnB,qBAAKF,KAAL,GAAa,KAAKtB,CAAlB;AACA,qBAAKuB,KAAL,GAAa,KAAKjC,CAAlB;AACA,qBAAKkC,KAAL,GAAa,KAAKL,CAAlB;AACH;AACD;AACA,gBAAImC,QAAQ9C,KAAK+C,GAAL,CAAS,KAAK3B,EAAL,GAAU,KAAKT,CAAxB,CAAZ;AACA,iBAAKqC,QAAL;;AAEA,gBAAGF,QAAQ,oBAAUrB,SAAlB,IAA+B,KAAKjB,EAAL,IAAW,oBAAUgB,aAAvD,EAAqE;AACjEyB,6BAAa,KAAK/B,KAAlB;AACA,qBAAK1B,CAAL,GAAS,KAAKsB,KAAd;AACA,qBAAKhC,CAAL,GAAS,KAAKiC,KAAd;AACA,qBAAKmC,QAAL;AACH,aALD,MAKO;AACH,qBAAKhC,KAAL,GAAaiC,WAAW,YAAM;AAC1B,2BAAK1D,KAAL,CAAW,OAAKC,iBAAhB;AACH,iBAFY,EAEV,oBAAU4B,aAFA,CAAb;AAGH;AACJ;;;;;;kBAKUiB,K;;;;;;;;;;;;;;;;ACtDf;;;;;;;;;;;;IAEMa,U;;;AAEF,wBAAYvE,EAAZ,EAAgBC,CAAhB,EAAmBC,QAAnB,EAA4B;AAAA;;AAAA,uHAClBF,EADkB,EACdC,CADc,EACXC,QADW,EACD,YADC;AAE3B;;;;gCAEO+C,I,EAAK;AACT,gBAAIuB,MAAM,EAAV;AACA,gBAAIC,UAAUxB,KAAKyB,KAAL,CAAW,CAAX,CAAd;AACA,mBAAMD,QAAQvD,MAAR,GAAiB,CAAvB,EAAyB;AACrB,oBAAIyD,QAAQxD,KAAKyD,KAAL,CAAWzD,KAAK0D,MAAL,KAAgBJ,QAAQvD,MAAnC,CAAZ;AACA,oBAAI4D,OAAOL,QAAQM,MAAR,CAAeJ,KAAf,EAAsB,CAAtB,EAAyB,CAAzB,CAAX;AACA,oBAAG,OAAOG,IAAP,IAAgB,WAAnB,EAAgC;AAC5BN,wBAAIR,IAAJ,CAASc,IAAT;AACH;AACJ;AACD,mBAAON,GAAP;AACH;;;8BAEKQ,C,EAAE;AAAA;;AACJ;AACA,iBAAKrD,EAAL;AACA;AACA,gBAAIsB,OAAO,KAAKgC,OAAL,CAAaD,CAAb,CAAX;AACA,gBAAMrB,IAAIV,KAAK/B,MAAf;;AAEA,iBAAI,IAAID,IAAI,CAAZ,EAAeA,IAAI0C,CAAnB,EAAsB1C,GAAtB,EAA2B;AACvB,oBAAMH,IAAImC,KAAKhC,CAAL,EAAQ,CAAR,CAAV;AACA,oBAAMP,IAAIuC,KAAKhC,CAAL,EAAQ,CAAR,CAAV;AACA,oBAAM6C,OAAQpD,IAAI,KAAKW,IAAL,CAAUP,CAAV,CAAlB;AACA;AACA,oBAAI8C,QAAQE,IAAZ;AACA,oBAAID,QAAQC,OAAOhD,CAAnB;AACA;AACA,qBAAKH,CAAL,GAAS,KAAKA,CAAL,GAAU,KAAKoB,CAAL,GAAS6B,KAA5B;AACA;AACA,qBAAK3D,CAAL,GAAS,KAAKA,CAAL,GAAU,EAAE,IAAE0D,CAAJ,KAAU,KAAK5B,CAAL,GAAS8B,KAAnB,CAAnB;AACH;AACD,iBAAKtB,EAAL,GAAU,KAAKT,CAAf;AACA;AACA,iBAAKA,CAAL,GAAS,KAAKiC,IAAL,CAAU,KAAKlD,iBAAf,CAAT;AACA,iBAAKN,WAAL,CAAiByD,IAAjB,CAAsB,KAAKlC,CAA3B;AACA;AACA,gBAAG,KAAKA,CAAL,GAAS,KAAKK,KAAjB,EAAuB;AACnB,qBAAKF,KAAL,GAAa,KAAKtB,CAAlB;AACA,qBAAKuB,KAAL,GAAa,KAAKjC,CAAlB;AACA,qBAAKkC,KAAL,GAAa,KAAKL,CAAlB;AACH;AACD;AACA,gBAAImC,QAAQ9C,KAAK+C,GAAL,CAAS,KAAK3B,EAAL,GAAU,KAAKT,CAAxB,CAAZ;AACA,iBAAKqC,QAAL;;AAEA,gBAAGF,QAAQ,oBAAUrB,SAAlB,IAA+B,KAAKjB,EAAL,IAAW,oBAAUgB,aAAvD,EAAqE;AACjEyB,6BAAa,KAAK/B,KAAlB;AACA,qBAAK1B,CAAL,GAAS,KAAKsB,KAAd;AACA,qBAAKhC,CAAL,GAAS,KAAKiC,KAAd;AACA,qBAAKmC,QAAL;AACH,aALD,MAKO;AACH,qBAAKhC,KAAL,GAAaiC,WAAW,YAAM;AAC1B,2BAAK1D,KAAL,CAAW,OAAKC,iBAAhB;AACH,iBAFY,EAEV,oBAAU4B,aAFA,CAAb;AAGH;AACJ;;;;;;kBAIU8B,U","file":"./dist/linear_regression/bgdVsgd/Worker.js","sourcesContent":[" \t// The module cache\n \tvar installedModules = {};\n\n \t// The require function\n \tfunction __webpack_require__(moduleId) {\n\n \t\t// Check if module is in cache\n \t\tif(installedModules[moduleId])\n \t\t\treturn installedModules[moduleId].exports;\n\n \t\t// Create a new module (and put it into the cache)\n \t\tvar module = installedModules[moduleId] = {\n \t\t\ti: moduleId,\n \t\t\tl: false,\n \t\t\texports: {}\n \t\t};\n\n \t\t// Execute the module function\n \t\tmodules[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n\n \t\t// Flag the module as loaded\n \t\tmodule.l = true;\n\n \t\t// Return the exports of the module\n \t\treturn module.exports;\n \t}\n\n\n \t// expose the modules object (__webpack_modules__)\n \t__webpack_require__.m = modules;\n\n \t// expose the module cache\n \t__webpack_require__.c = installedModules;\n\n \t// identity function for calling harmony imports with the correct context\n \t__webpack_require__.i = function(value) { return value; };\n\n \t// define getter function for harmony exports\n \t__webpack_require__.d = function(exports, name, getter) {\n \t\tif(!__webpack_require__.o(exports, name)) {\n \t\t\tObject.defineProperty(exports, name, {\n \t\t\t\tconfigurable: false,\n \t\t\t\tenumerable: true,\n \t\t\t\tget: getter\n \t\t\t});\n \t\t}\n \t};\n\n \t// getDefaultExport function for compatibility with non-harmony modules\n \t__webpack_require__.n = function(module) {\n \t\tvar getter = module && module.__esModule ?\n \t\t\tfunction getDefault() { return module['default']; } :\n \t\t\tfunction getModuleExports() { return module; };\n \t\t__webpack_require__.d(getter, 'a', getter);\n \t\treturn getter;\n \t};\n\n \t// Object.prototype.hasOwnProperty.call\n \t__webpack_require__.o = function(object, property) { return Object.prototype.hasOwnProperty.call(object, property); };\n\n \t// __webpack_public_path__\n \t__webpack_require__.p = \"\";\n\n \t// Load entry module and return exports\n \treturn __webpack_require__(__webpack_require__.s = 4);\n\n\n\n// WEBPACK FOOTER //\n// webpack/bootstrap 8d5dc9a55c94b1145e88","class Regressor{\n\n\n    static NUM_POINTS = 50;\n    static TIME_INTERVAL = 10;\n    static RANDOM_ALPHA = .5;\n    static LEARNING_RATE = 0.001;\n    static MAX_ITERATION = 1000;\n    static PRECISION = 0.001;\n\n    constructor(rn, m, batchNum, objectName){\n        this.real_numbers = rn;\n        this.measurements = m;\n        this.batchNum = batchNum;\n        this.object_name = objectName;\n        this.costHistory = [];\n        this.epocPost = 0;\n        this.setup();\n    }\n\n    start(y){\n        this.b = y;\n        this.epoch(this.training_examples);\n    }\n\n    line(x){\n        return this.b + this.m * x;\n    }\n\n    cost(points) {\n        let error = 0, i, x, y;\n        for (i = 0; i < points.length; i++) {\n            x = points[i][0];\n            y = points[i][1];\n            error += Math.pow((y - this.line(x)), 2);\n        }\n        return error / points.length;\n    }\n\n    getLineData(){\n        return this.real_numbers.map((n, i) => {\n            return this.line(i);\n        });\n    }\n\n    drawCost(){\n        postMessage({\n            action: 'DRAW_COST',\n            objectName: this.object_name,\n            batchNum: this.batchNum,\n            costHistory: this.costHistory,\n            epocNum: this.ni\n        });\n    }\n\n    drawLine(){\n        if(this.epocPost % 10 == 0) {\n            const lineData = this.getLineData();\n\n            postMessage({\n                action: 'DRAW_LINE',\n                lineData: lineData,\n                batchNum: this.batchNum,\n                epocNum: this.ni\n            });\n        }\n        this.epocPost++;\n    }\n\n    setup(){\n        /**\n         * Training examples is the [x, y] pairs that will be used for training\n         */\n        this.training_examples = this.measurements\n            .map((n, i) => {\n                return [i, n];\n            });\n        /**\n         * Number of iterations\n         * @type {number}\n         */\n        this.ni = 0;\n        /**\n         * Current Cost\n         * @type {number}\n         */\n        this.c = 0;\n        /**\n         * Alpha - Learning rate\n         * @type {number}\n         */\n        this.a = Regressor.LEARNING_RATE;\n        /**\n         * Best B - after convergence we choose the best B value\n         * @type {number}\n         */\n        this.bestB = 0;\n        /**\n         * Bese M - after convergence we choose the best M value\n         * @type {number}\n         */\n        this.bestM = 0;\n        /**\n         * Best Cost - keep track of the best cost we have reached so far\n         * @type {Number}\n         */\n        this.bestC = Infinity;\n        /**\n         * The slope\n         * @type {number}\n         */\n        this.m = 0;\n        /**\n         * The timer\n         * @type {null}\n         */\n        this.timer = null;\n        /**\n         * The HighCharts reference\n         * @type {Highcharts.Chart}\n         */\n        this.hc = null;\n        /**\n         * Previous cost, used for measuring precision\n         * @type {Number}\n         */\n        this.pc = Infinity;\n    }\n}\n\nexport default Regressor;\n\n\n// WEBPACK FOOTER //\n// ./src/linear_regression/bgdVsgd/Regressor.es6","import Batch from './Batch';\nimport Stochastic from './Stochastic';\n\n\nclass Worker{\n\n    handleMessage(e){\n        console.log('Message received from main script');\n        console.log(e.data);\n        switch(e.data.action) {\n            case 'SET_WORKER_VARS':\n                switch(e.data.workerType){\n                    case \"BATCH\":\n                        this.worker = new Batch(e.data.measurements, e.data.realNumbers, e.data.batchNum);\n                        break;\n\n                    case \"STOCHASTIC\":\n                        this.worker = new Stochastic(e.data.measurements, e.data.realNumbers, e.data.batchNum);\n                        break;\n                }\n                this.worker.start(e.data.startY);\n                break;\n        }\n    }\n}\n\nconst w = new Worker();\n\nonmessage = (e) => {\n    w.handleMessage(e);\n};\n\n\n\n// WEBPACK FOOTER //\n// ./src/linear_regression/bgdVsgd/Worker.es6","import Regressor from './Regressor';\n\nclass Batch extends Regressor {\n\n    constructor(rn, m, batchNum){\n        super(rn, m, batchNum, \"Batch\");\n    }\n\n    epoch(data){\n        //keep track of how many times we go through the dataset\n        this.ni++;\n        const N = data.length;\n        let bGrad = 0;\n        let mGrad = 0;\n        for(let i = 0; i < N; i ++){\n            const x = data[i][0];\n            const y = data[i][1];\n            const diff = (y - this.line(x));\n\n            //calculate the gradients\n            bGrad += diff;\n            mGrad += diff * x;\n        }\n        this.b = this.b + (this.a * bGrad);\n        this.m = this.m - (-(2/N) * (this.a * mGrad));\n        this.pc = this.c;\n        //compute the mean squared error\n        this.c = this.cost(this.training_examples);\n        this.costHistory.push(this.c);\n        //keep track of the best possible answer\n        if(this.c < this.bestC){\n            this.bestB = this.b;\n            this.bestM = this.m;\n            this.bestC = this.c;\n        }\n        //compute the change in cost\n        let ccost = Math.abs(this.pc - this.c);\n        this.drawLine();\n\n        if(ccost < Regressor.PRECISION || this.ni >= Regressor.MAX_ITERATION){\n            clearTimeout(this.timer);\n            this.b = this.bestB;\n            this.m = this.bestM;\n            this.drawCost();\n        } else {\n            this.timer = setTimeout(() => {\n                this.epoch(this.training_examples);\n            }, Regressor.TIME_INTERVAL);\n        }\n    }\n\n}\n\n\nexport default Batch;\n\n\n\n\n// WEBPACK FOOTER //\n// ./src/linear_regression/bgdVsgd/Batch.es6","import Regressor from './Regressor';\n\nclass Stochastic extends Regressor {\n\n    constructor(rn, m, batchNum){\n        super(rn, m, batchNum, \"Stochastic\");\n    }\n\n    shuffle(data){\n        let ret = [];\n        let tmpData = data.slice(0);\n        while(tmpData.length > 0){\n            let index = Math.round(Math.random() * tmpData.length);\n            let item = tmpData.splice(index, 1)[0];\n            if(typeof(item) != \"undefined\") {\n                ret.push(item);\n            }\n        }\n        return ret;\n    }\n\n    epoch(d){\n        //keep track of how many times we go through the dataset\n        this.ni++;\n        //Shuffle our data first\n        let data = this.shuffle(d);\n        const N = data.length;\n\n        for(let i = 0; i < N; i ++){\n            const x = data[i][0];\n            const y = data[i][1];\n            const diff = (y - this.line(x));\n            //calculate the gradients for a single sample\n            let bGrad = diff;\n            let mGrad = diff * x;\n            //update Y intercept\n            this.b = this.b + (this.a * bGrad);\n            //update slope\n            this.m = this.m - (-(2/N) * (this.a * mGrad));\n        }\n        this.pc = this.c;\n        //compute the mean squared error\n        this.c = this.cost(this.training_examples);\n        this.costHistory.push(this.c);\n        //keep track of the best possible answer\n        if(this.c < this.bestC){\n            this.bestB = this.b;\n            this.bestM = this.m;\n            this.bestC = this.c;\n        }\n        //compute the change in cost\n        let ccost = Math.abs(this.pc - this.c);\n        this.drawLine();\n\n        if(ccost < Regressor.PRECISION || this.ni >= Regressor.MAX_ITERATION){\n            clearTimeout(this.timer);\n            this.b = this.bestB;\n            this.m = this.bestM;\n            this.drawCost();\n        } else {\n            this.timer = setTimeout(() => {\n                this.epoch(this.training_examples);\n            }, Regressor.TIME_INTERVAL);\n        }\n    }\n}\n\n\nexport default Stochastic;\n\n\n\n\n// WEBPACK FOOTER //\n// ./src/linear_regression/bgdVsgd/Stochastic.es6"],"sourceRoot":""}